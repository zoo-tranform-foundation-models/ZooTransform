{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Load packages and Set Working Directory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ffbd20a09157395"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "data_root = os.path.join(parent_dir, \"ZooTransform\")\n",
    "os.chdir(data_root)\n",
    "\n",
    "from src.model.species_model import SpeciesAwareESM2"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-05T00:47:53.062847Z",
     "start_time": "2025-11-05T00:47:53.058349Z"
    }
   },
   "id": "initial_id",
   "execution_count": 83
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load and Use SpeciesAwareESM2 Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1d363ef1d71c6ca"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Using device: cuda\n",
      "  GPU: NVIDIA A100-SXM4-40GB\n",
      "  Memory: 42.29 GB\n",
      "ðŸ“¥ Loading model: facebook/esm2_t6_8M_UR50D\n",
      "Adding species tokens: ['<sp_human>', '<sp_mouse>', '<sp_ecoli>']\n",
      "Added 3 new special tokens\n",
      "Resized model embeddings to 36 tokens\n",
      "âœ“ Model and tokenizer ready!\n",
      "  Hidden size: 320\n",
      "  Number of layers: 6\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained SpeciesAwareESM2 model\n",
    "model = SpeciesAwareESM2(model_name=\"facebook/esm2_t6_8M_UR50D\", species_list=[\"human\", \"mouse\", \"ecoli\"]) #TODO - define species list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-05T00:47:53.444246Z",
     "start_time": "2025-11-05T00:47:53.065258Z"
    }
   },
   "id": "b2d9bebce5381e5d",
   "execution_count": 84
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "439834042c942c62"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Example data for embedding - need to replace with our actual data\n",
    "data = pd.DataFrame({\n",
    "    \"species\": [\"human\", \"mouse\", \"ecoli\"],\n",
    "    \"sequence\": [\n",
    "        \"MKTAYIAKQRQISFVKSHFSRQLEERLGLIEVQ\",\n",
    "        \"MKVSAIAKQRQISFVKSHFSRQLRERLGLIEVQ\",\n",
    "        \"MKTVYIAKQRQISFVKSHFSRQLEERLGLIEVQ\"\n",
    "    ]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-05T00:47:53.449552Z",
     "start_time": "2025-11-05T00:47:53.445829Z"
    }
   },
   "id": "926a9d089f2b0d0f",
   "execution_count": 85
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate Embeddings for Sequences with Species Information, with mean pooling\n",
    "slower , not recommended for a large dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db35e26934804018"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings array shape: (3, 320)\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for each sequence manually\n",
    "embeddings = []\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    emb = model.embed(row['species'], row['sequence'])\n",
    "    emb_mean = emb.mean(dim=1).squeeze().cpu().numpy() # Mean pooling over sequence length, can decide to use different pooling\n",
    "    embeddings.append(emb_mean)\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "print(\"Embeddings array shape:\", embeddings.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-05T00:47:53.481739Z",
     "start_time": "2025-11-05T00:47:53.450982Z"
    }
   },
   "id": "dcf80d8cbcd2a77d",
   "execution_count": 86
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate Embeddings for *a Batch of Sequences* with Species Information, with mean pooling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f9014b9a4dbec62"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 320)\n"
     ]
    }
   ],
   "source": [
    "species_batch = data['species'].tolist()\n",
    "sequence_batch = data['sequence'].tolist()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.forward(species_batch, sequence_batch)\n",
    "\n",
    "batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "print(batch_embeddings.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-05T00:47:53.496317Z",
     "start_time": "2025-11-05T00:47:53.483869Z"
    }
   },
   "id": "414650b304c22b24",
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "embeddings\n",
      "embeddings.word_embeddings\n",
      "embeddings.dropout\n",
      "embeddings.position_embeddings\n",
      "encoder\n",
      "encoder.layer\n",
      "encoder.layer.0\n",
      "encoder.layer.0.attention\n",
      "encoder.layer.0.attention.self\n",
      "encoder.layer.0.attention.self.query\n",
      "encoder.layer.0.attention.self.key\n",
      "encoder.layer.0.attention.self.value\n",
      "encoder.layer.0.attention.self.dropout\n",
      "encoder.layer.0.attention.self.rotary_embeddings\n",
      "encoder.layer.0.attention.output\n",
      "encoder.layer.0.attention.output.dense\n",
      "encoder.layer.0.attention.output.dropout\n",
      "encoder.layer.0.attention.LayerNorm\n",
      "encoder.layer.0.intermediate\n",
      "encoder.layer.0.intermediate.dense\n",
      "encoder.layer.0.output\n",
      "encoder.layer.0.output.dense\n",
      "encoder.layer.0.output.dropout\n",
      "encoder.layer.0.LayerNorm\n",
      "encoder.layer.1\n",
      "encoder.layer.1.attention\n",
      "encoder.layer.1.attention.self\n",
      "encoder.layer.1.attention.self.query\n",
      "encoder.layer.1.attention.self.key\n",
      "encoder.layer.1.attention.self.value\n",
      "encoder.layer.1.attention.self.dropout\n",
      "encoder.layer.1.attention.self.rotary_embeddings\n",
      "encoder.layer.1.attention.output\n",
      "encoder.layer.1.attention.output.dense\n",
      "encoder.layer.1.attention.output.dropout\n",
      "encoder.layer.1.attention.LayerNorm\n",
      "encoder.layer.1.intermediate\n",
      "encoder.layer.1.intermediate.dense\n",
      "encoder.layer.1.output\n",
      "encoder.layer.1.output.dense\n",
      "encoder.layer.1.output.dropout\n",
      "encoder.layer.1.LayerNorm\n",
      "encoder.layer.2\n",
      "encoder.layer.2.attention\n",
      "encoder.layer.2.attention.self\n",
      "encoder.layer.2.attention.self.query\n",
      "encoder.layer.2.attention.self.key\n",
      "encoder.layer.2.attention.self.value\n",
      "encoder.layer.2.attention.self.dropout\n",
      "encoder.layer.2.attention.self.rotary_embeddings\n",
      "encoder.layer.2.attention.output\n",
      "encoder.layer.2.attention.output.dense\n",
      "encoder.layer.2.attention.output.dropout\n",
      "encoder.layer.2.attention.LayerNorm\n",
      "encoder.layer.2.intermediate\n",
      "encoder.layer.2.intermediate.dense\n",
      "encoder.layer.2.output\n",
      "encoder.layer.2.output.dense\n",
      "encoder.layer.2.output.dropout\n"
     ]
    }
   ],
   "source": [
    "for name, module in list(model.model.named_modules())[:60]:\n",
    "    print(name)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-05T00:47:53.501863Z",
     "start_time": "2025-11-05T00:47:53.497471Z"
    }
   },
   "id": "3f11adf492f0cf66",
   "execution_count": 88
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine-tune the Model using LoRA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "878d650a87ec3b52"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Using device: cuda\n",
      "  GPU: NVIDIA A100-SXM4-40GB\n",
      "  Memory: 42.29 GB\n",
      "ðŸ“¥ Loading model: facebook/esm2_t6_8M_UR50D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding species tokens: ['<sp_human>', '<sp_mouse>', '<sp_ecoli>']\n",
      "Added 3 new special tokens\n",
      "Resized model embeddings to 36 tokens\n",
      "âœ“ Model and tokenizer ready!\n",
      "  Hidden size: 320\n",
      "  Number of layers: 6\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target modules {'q_proj', 'v_proj'} not found in the base model. Please check the target modules and try again.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[90]\u001B[39m\u001B[32m, line 12\u001B[39m\n\u001B[32m      9\u001B[39m sequence_batch = data[\u001B[33m\"\u001B[39m\u001B[33msequence\u001B[39m\u001B[33m\"\u001B[39m].tolist()\n\u001B[32m     11\u001B[39m \u001B[38;5;66;03m# ---- Create and train LoRA finetuner ----\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m finetuner = \u001B[43mLoraESMFinetuner\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbase_model\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[43mr\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m8\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43malpha\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m16\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.05\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1e-4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmlm_probability\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.15\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     22\u001B[39m finetuner.train(species_batch, sequence_batch, epochs=\u001B[32m10\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/workspace/ZooTransform/src/fine_tuning/fine_tuning.py:50\u001B[39m, in \u001B[36mLoraESMFinetuner.__init__\u001B[39m\u001B[34m(self, base_model, r, alpha, dropout, target_modules, lr, batch_size, mlm_probability)\u001B[39m\n\u001B[32m     40\u001B[39m     target_modules = [\u001B[33m\"\u001B[39m\u001B[33mq_proj\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mv_proj\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m     42\u001B[39m lora_cfg = LoraConfig(\n\u001B[32m     43\u001B[39m     r=r,\n\u001B[32m     44\u001B[39m     lora_alpha=alpha,\n\u001B[32m   (...)\u001B[39m\u001B[32m     48\u001B[39m     task_type=\u001B[33m\"\u001B[39m\u001B[33mCAUSAL_LM\u001B[39m\u001B[33m\"\u001B[39m,  \u001B[38;5;66;03m# ESM is a language model\u001B[39;00m\n\u001B[32m     49\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m50\u001B[39m \u001B[38;5;28mself\u001B[39m.model = \u001B[43mget_peft_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlora_cfg\u001B[49m\u001B[43m)\u001B[49m.to(\u001B[38;5;28mself\u001B[39m.device)\n\u001B[32m     52\u001B[39m \u001B[38;5;28mself\u001B[39m.optimizer = AdamW(\u001B[38;5;28mself\u001B[39m.model.parameters(), lr=lr)\n\u001B[32m     53\u001B[39m \u001B[38;5;28mself\u001B[39m.data_collator = DataCollatorForLanguageModeling(\n\u001B[32m     54\u001B[39m     tokenizer=\u001B[38;5;28mself\u001B[39m.tokenizer, mlm=\u001B[38;5;28;01mTrue\u001B[39;00m, mlm_probability=\u001B[38;5;28mself\u001B[39m.mlm_probability\n\u001B[32m     55\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/venv/lib/python3.12/site-packages/peft/mapping.py:193\u001B[39m, in \u001B[36mget_peft_model\u001B[39m\u001B[34m(model, peft_config, adapter_name, mixed, autocast_adapter_dtype, revision)\u001B[39m\n\u001B[32m    191\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m peft_config.is_prompt_learning:\n\u001B[32m    192\u001B[39m     peft_config = _prepare_prompt_learning_config(peft_config, model_config)\n\u001B[32m--> \u001B[39m\u001B[32m193\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001B[49m\u001B[43m[\u001B[49m\u001B[43mpeft_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtask_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    194\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpeft_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mautocast_adapter_dtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mautocast_adapter_dtype\u001B[49m\n\u001B[32m    195\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/venv/lib/python3.12/site-packages/peft/peft_model.py:1609\u001B[39m, in \u001B[36mPeftModelForCausalLM.__init__\u001B[39m\u001B[34m(self, model, peft_config, adapter_name, **kwargs)\u001B[39m\n\u001B[32m   1606\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\n\u001B[32m   1607\u001B[39m     \u001B[38;5;28mself\u001B[39m, model: torch.nn.Module, peft_config: PeftConfig, adapter_name: \u001B[38;5;28mstr\u001B[39m = \u001B[33m\"\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m\"\u001B[39m, **kwargs\n\u001B[32m   1608\u001B[39m ) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1609\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpeft_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1610\u001B[39m     \u001B[38;5;28mself\u001B[39m.base_model_prepare_inputs_for_generation = \u001B[38;5;28mself\u001B[39m.base_model.prepare_inputs_for_generation\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/venv/lib/python3.12/site-packages/peft/peft_model.py:171\u001B[39m, in \u001B[36mPeftModel.__init__\u001B[39m\u001B[34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001B[39m\n\u001B[32m    169\u001B[39m     ctx = init_empty_weights \u001B[38;5;28;01mif\u001B[39;00m low_cpu_mem_usage \u001B[38;5;28;01melse\u001B[39;00m nullcontext\n\u001B[32m    170\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx():\n\u001B[32m--> \u001B[39m\u001B[32m171\u001B[39m         \u001B[38;5;28mself\u001B[39m.base_model = \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpeft_config\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    172\u001B[39m     \u001B[38;5;28mself\u001B[39m.set_additional_trainable_modules(peft_config, adapter_name)\n\u001B[32m    174\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.base_model, \u001B[33m\"\u001B[39m\u001B[33m_cast_adapter_dtype\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/venv/lib/python3.12/site-packages/peft/tuners/lora/model.py:141\u001B[39m, in \u001B[36mLoraModel.__init__\u001B[39m\u001B[34m(self, model, config, adapter_name, low_cpu_mem_usage)\u001B[39m\n\u001B[32m    140\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, model, config, adapter_name, low_cpu_mem_usage: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m141\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:184\u001B[39m, in \u001B[36mBaseTuner.__init__\u001B[39m\u001B[34m(self, model, peft_config, adapter_name, low_cpu_mem_usage)\u001B[39m\n\u001B[32m    182\u001B[39m \u001B[38;5;28mself\u001B[39m._pre_injection_hook(\u001B[38;5;28mself\u001B[39m.model, \u001B[38;5;28mself\u001B[39m.peft_config[adapter_name], adapter_name)\n\u001B[32m    183\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m peft_config != PeftType.XLORA \u001B[38;5;129;01mor\u001B[39;00m peft_config[adapter_name] != PeftType.XLORA:\n\u001B[32m--> \u001B[39m\u001B[32m184\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minject_adapter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    186\u001B[39m \u001B[38;5;66;03m# Copy the peft_config in the injected model.\u001B[39;00m\n\u001B[32m    187\u001B[39m \u001B[38;5;28mself\u001B[39m.model.peft_config = \u001B[38;5;28mself\u001B[39m.peft_config\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:509\u001B[39m, in \u001B[36mBaseTuner.inject_adapter\u001B[39m\u001B[34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001B[39m\n\u001B[32m    507\u001B[39m \u001B[38;5;66;03m# Handle X-LoRA case.\u001B[39;00m\n\u001B[32m    508\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_target_modules_in_base_model \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(peft_config, \u001B[33m\"\u001B[39m\u001B[33mtarget_modules\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m509\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    510\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTarget modules \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpeft_config.target_modules\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m not found in the base model. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    511\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mPlease check the target modules and try again.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    512\u001B[39m     )\n\u001B[32m    514\u001B[39m \u001B[38;5;66;03m# It's important to set the adapter here (again), because otherwise it can happen that if a 2nd adapter is\u001B[39;00m\n\u001B[32m    515\u001B[39m \u001B[38;5;66;03m# added, and it targets different layer(s) than the first adapter (which is active), then those different\u001B[39;00m\n\u001B[32m    516\u001B[39m \u001B[38;5;66;03m# layers will be activated, which we don't want.\u001B[39;00m\n\u001B[32m    517\u001B[39m \u001B[38;5;28mself\u001B[39m.set_adapter(\u001B[38;5;28mself\u001B[39m.active_adapters)\n",
      "\u001B[31mValueError\u001B[39m: Target modules {'q_proj', 'v_proj'} not found in the base model. Please check the target modules and try again."
     ]
    }
   ],
   "source": [
    "from src.fine_tuning.fine_tuning import LoraESMFinetuner\n",
    "\n",
    "# ---- Instantiate base model ----\n",
    "species_list = [\"human\", \"mouse\", \"ecoli\"] #TODO - define species list\n",
    "model = SpeciesAwareESM2(species_list=species_list)\n",
    "\n",
    "# ---- Prepare your data ----\n",
    "species_batch = data[\"species\"].tolist()\n",
    "sequence_batch = data[\"sequence\"].tolist()\n",
    "\n",
    "# ---- Create and train LoRA finetuner ----\n",
    "finetuner = LoraESMFinetuner(\n",
    "    base_model=model,\n",
    "    r=8,\n",
    "    alpha=16,\n",
    "    dropout=0.05,\n",
    "    lr=1e-4,\n",
    "    batch_size=4,\n",
    "    mlm_probability=0.15,\n",
    ")\n",
    "\n",
    "finetuner.train(species_batch, sequence_batch, epochs=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-05T00:50:43.210858Z",
     "start_time": "2025-11-05T00:50:42.739769Z"
    }
   },
   "id": "f46046353053052f",
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tuned_embeddings = finetuner.embed(species_batch, sequence_batch)\n",
    "\n",
    "print(\"Tuned embedding shape:\", tuned_embeddings.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5ac590c191d6401"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
