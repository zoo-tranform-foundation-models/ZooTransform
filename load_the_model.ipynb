{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-04T23:10:05.758590Z",
     "start_time": "2025-11-04T23:10:04.901276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Using device: cuda\n",
      "  GPU: NVIDIA A100-SXM4-40GB\n",
      "  Memory: 42.29 GB\n",
      "ðŸ“¥ Loading model: facebook/esm2_t6_8M_UR50D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding species tokens: ['<sp_human>', '<sp_mouse>', '<sp_ecoli>']\n",
      "Added 3 new special tokens\n",
      "Resized model embeddings to 36 tokens\n",
      "âœ“ Model and tokenizer ready!\n",
      "  Hidden size: 320\n",
      "  Number of layers: 6\n",
      "Embeddings array shape: (3, 320)\n",
      "(3, 320)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "data_root = os.path.join(parent_dir, \"ZooTransform\")\n",
    "os.chdir(data_root)\n",
    "\n",
    "from src.model.species_model import SpeciesAwareESM2\n",
    "\n",
    "# Load the pre-trained SpeciesAwareESM2 model\n",
    "model = SpeciesAwareESM2(model_name=\"facebook/esm2_t6_8M_UR50D\", species_list=[\"human\", \"mouse\", \"ecoli\"])\n",
    "\n",
    "# Example data for embedding - need to replace with our actual data\n",
    "data = pd.DataFrame({\n",
    "    \"species\": [\"human\", \"mouse\", \"ecoli\"],\n",
    "    \"sequence\": [\n",
    "        \"MKTAYIAKQRQISFVKSHFSRQLEERLGLIEVQ\",\n",
    "        \"MKVSAIAKQRQISFVKSHFSRQLRERLGLIEVQ\",\n",
    "        \"MKTVYIAKQRQISFVKSHFSRQLEERLGLIEVQ\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    emb = model.embed(row['species'], row['sequence'])\n",
    "    emb_mean = emb.mean(dim=1).squeeze().cpu().numpy() # Mean pooling over sequence length, can decide to use different pooling\n",
    "    embeddings.append(emb_mean)\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "print(\"Embeddings array shape:\", embeddings.shape)\n",
    "\n",
    "species_batch = data['species'].tolist()\n",
    "sequence_batch = data['sequence'].tolist()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.forward(species_batch, sequence_batch)\n",
    "\n",
    "batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "print(batch_embeddings.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dcf80d8cbcd2a77d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
