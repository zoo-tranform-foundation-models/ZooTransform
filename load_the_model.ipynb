{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ffbd20a09157395",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load packages and Set Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T15:37:17.743046Z",
     "start_time": "2025-11-05T15:37:17.740086Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hslab/Olive/Kode/ZooTransform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hslab/Olive/Kode/ZooTransform/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "print(os.getcwd())\n",
    "#parent_dir = os.path.dirname(os.getcwd())\n",
    "#print(parent_dir)\n",
    "data_root = os.path.join(os.getcwd(), 'ZooTransform')\n",
    "# os.chdir(data_root)\n",
    "\n",
    "sys.path.append(data_root)\n",
    "\n",
    "from src.zootransform.model.species_model import SpeciesAwareESM2\n",
    "from src.zootransform.dataset.load_uniprot import load_uniprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c20bf6a9aab41a15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T15:37:17.967094Z",
     "start_time": "2025-11-05T15:37:17.771056Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t     load_the_model.ipynb  requirements.txt  uniprot_data\n",
      "Training_data.ipynb  optuna.ipynb\t   setup.cfg\t     validations.ipynb\n",
      "UQ.ipynb\t     pyproject.toml\t   src\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439834042c942c62",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "926a9d089f2b0d0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T15:37:18.335622Z",
     "start_time": "2025-11-05T15:37:18.329397Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arabidopsis thaliana',\n",
       " 'Bos taurus',\n",
       " 'Escherichia coli',\n",
       " 'Homo sapiens',\n",
       " 'Mus musculus',\n",
       " 'Oryza sativa',\n",
       " 'Rattus norvegicus',\n",
       " 'Rhodotorula toruloides',\n",
       " 'Saccharolobus solfataricus',\n",
       " 'Saccharomyces cerevisiae',\n",
       " 'Schizosaccharomyces pombe',\n",
       " 'Staphylococcus aureus']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example data for embedding - need to replace with our actual data\n",
    "data = load_uniprot()\n",
    "species_names = sorted(set(data['species'].unique().tolist()))\n",
    "species_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d363ef1d71c6ca",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load and Use SpeciesAwareESM2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2d9bebce5381e5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T15:37:18.327369Z",
     "start_time": "2025-11-05T15:37:17.968522Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "  GPU: NVIDIA GeForce RTX 4090\n",
      "  Memory: 25.39 GB\n",
      "Loading model: facebook/esm2_t6_8M_UR50D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding species tokens: ['<sp_Arabidopsis thaliana>', '<sp_Bos taurus>', '<sp_Escherichia coli>', '<sp_Homo sapiens>', '<sp_Mus musculus>', '<sp_Oryza sativa>', '<sp_Rattus norvegicus>', '<sp_Rhodotorula toruloides>', '<sp_Saccharolobus solfataricus>', '<sp_Saccharomyces cerevisiae>', '<sp_Schizosaccharomyces pombe>', '<sp_Staphylococcus aureus>']\n",
      "Added 12 new special tokens\n",
      "Resized model embeddings to 45 tokens\n",
      "✓ Model and tokenizer ready!\n",
      "  Hidden size: 320\n",
      "  Number of layers: 6\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained SpeciesAwareESM2 model\n",
    "species_model = SpeciesAwareESM2(model_name=\"facebook/esm2_t6_8M_UR50D\", species_list=species_names) #TODO - define species list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db35e26934804018",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Generate Embeddings for Sequences with Species Information, with mean pooling\n",
    "slower , not recommended for a large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf80d8cbcd2a77d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T15:37:18.407846Z",
     "start_time": "2025-11-05T15:37:18.337838Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings array shape: (120095, 320)\n"
     ]
    }
   ],
   "source": [
    "# Load or generate embeddings\n",
    "embeddings_file = \"uniprot_embeddings.npy\"\n",
    "is_load_embeddings = True  # Set to True to load existing embeddings if available\n",
    "\n",
    "if os.path.exists(embeddings_file) and is_load_embeddings:\n",
    "    embeddings = np.load(embeddings_file)\n",
    "    print(f\"Loaded embeddings from {embeddings_file}, shape: {embeddings.shape}\")\n",
    "else:\n",
    "    # Generate embeddings for each sequence manually\n",
    "    embeddings = []\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        emb = species_model.embed(row['species'], row['sequence'])\n",
    "        emb_mean = emb.mean(dim=1).squeeze().cpu().numpy() # Mean pooling over sequence length, can decide to use different pooling\n",
    "        embeddings.append(emb_mean)\n",
    "\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    print(\"Embeddings array shape:\", embeddings.shape)\n",
    "    \n",
    "    # Save embeddings if they haven't already\n",
    "    embeddings_file = \"uniprot_embeddings.npy\"\n",
    "    if not os.path.exists(embeddings_file):\n",
    "        np.save(embeddings_file, embeddings)\n",
    "        print(f\"Embeddings saved to {embeddings_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9014b9a4dbec62",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Generate Embeddings for *a Batch of Sequences* with Species Information, with mean pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414650b304c22b24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T15:37:18.418756Z",
     "start_time": "2025-11-05T15:37:18.408611Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate embeddings for a batch of sequences (faster); here batch is the entire dataset\n",
    "species_batch = data[\"species\"].tolist()\n",
    "sequence_batch = data[\"sequence\"].tolist()\n",
    "\n",
    "is_run_batch = False\n",
    "if is_run_batch:\n",
    "    with torch.no_grad():\n",
    "        outputs = species_model.forward(species_batch, sequence_batch)\n",
    "\n",
    "    batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    print(batch_embeddings.shape)\n",
    "else:\n",
    "    batch_embeddings = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f11adf492f0cf66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T15:37:18.421294Z",
     "start_time": "2025-11-05T15:37:18.419503Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for name, module in list(species_model.model.named_modules())[:60]:\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404cdfa2ef9c53df",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Split Data into Train, Validation, and Test Sets (not used in our case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b5c38fd0eb91a39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T15:37:18.425859Z",
     "start_time": "2025-11-05T15:37:18.422112Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 84066, Val: 18014, Test: 18015\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split #TODO - do we want to have splits? generally we just want to fine-tune on all data\n",
    "\n",
    "# First, split train vs temp (validation + test)\n",
    "species_train, species_temp, seq_train, seq_temp = train_test_split(\n",
    "    species_batch, sequence_batch, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Then, split temp into validation and test (50% of temp each = 15% total)\n",
    "species_val, species_test, seq_val, seq_test = train_test_split(\n",
    "    species_temp, seq_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(species_train)}, Val: {len(species_val)}, Test: {len(species_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878d650a87ec3b52",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fine-tune the Model using LoRA (on all data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ddba2dbc52030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T15:37:19.369148Z",
     "start_time": "2025-11-05T15:37:18.426629Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m species_batch \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecies\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()[:max_len]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_size)\n\u001b[1;32m     17\u001b[0m sequence_batch \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()[:max_len]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_size)\n\u001b[0;32m---> 18\u001b[0m tokens_old \u001b[38;5;241m=\u001b[39m \u001b[43mtokens_old\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_size)\n\u001b[1;32m     20\u001b[0m tokens_old \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m tokens_old\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "from src.zootransform.fine_tuning.fine_tuning import LoraFinetuner, LoraFinetunerMLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "species_batch = data[\"species\"].tolist()\n",
    "sequence_batch = data[\"sequence\"].tolist()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "old_model = AutoModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\").to(device)\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "tokens_old = old_tokenizer(sequence_batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "\n",
    "# Reshape\n",
    "batch_size = 128\n",
    "max_len = len(data) - np.mod(len(data), batch_size)\n",
    "species_batch = data[\"species\"].to_numpy()[:max_len].reshape(-1, batch_size)\n",
    "sequence_batch = data[\"sequence\"].to_numpy()[:max_len].reshape(-1, batch_size)\n",
    "# tokens_old = tokens_old[:max_len].reshape(-1, batch_size)\n",
    "\n",
    "tokens_old = {k: v[:max_len].reshape(-1, batch_size).to(device) for k,v in tokens_old.items()}\n",
    "with torch.no_grad():\n",
    "    old_embeddings = old_model(**tokens_old).last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Species-aware model\n",
    "species_model = SpeciesAwareESM2(species_list=species_names)\n",
    "\n",
    "# LoRA finetuner\n",
    "finetuner = LoraFinetuner(\n",
    "    base_model=species_model, \n",
    "    r=8, \n",
    "    alpha=16, \n",
    "    dropout=0.05, \n",
    "    target_modules=None, \n",
    "    lr=1e-4, \n",
    "    batch_size=4)  #TODO - optionally optimize parameters for LoRA \n",
    "\n",
    "# Train to align species embeddings to frozen embeddings\n",
    "finetuner.train(species_train, seq_train, frozen_embeddings=None, epochs=5) #TODO - set frozen embeddings\n",
    "\n",
    "# Extract tuned embeddings\n",
    "tuned_embeddings = finetuner.embed(species_batch, sequence_batch)\n",
    "print(\"Tuned embeddings shape:\", tuned_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efe0d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "  GPU: NVIDIA GeForce RTX 4090\n",
      "  Memory: 25.39 GB\n",
      "Loading model: facebook/esm2_t6_8M_UR50D\n",
      "Adding species tokens: ['<sp_Arabidopsis thaliana>', '<sp_Bos taurus>', '<sp_Escherichia coli>', '<sp_Homo sapiens>', '<sp_Mus musculus>', '<sp_Oryza sativa>', '<sp_Rattus norvegicus>', '<sp_Rhodotorula toruloides>', '<sp_Saccharolobus solfataricus>', '<sp_Saccharomyces cerevisiae>', '<sp_Schizosaccharomyces pombe>', '<sp_Staphylococcus aureus>']\n",
      "Added 12 new special tokens\n",
      "Resized model embeddings to 45 tokens\n",
      "✓ Model and tokenizer ready!\n",
      "  Hidden size: 320\n",
      "  Number of layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/21017 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MaskedLMOutput' object has no attribute 'last_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 19\u001b[0m\n\u001b[1;32m      9\u001b[0m finetuner \u001b[38;5;241m=\u001b[39m LoraFinetuner(\n\u001b[1;32m     10\u001b[0m     base_model\u001b[38;5;241m=\u001b[39mspecies_model, \n\u001b[1;32m     11\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, \n\u001b[1;32m     16\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)  \u001b[38;5;66;03m#TODO - optionally optimize parameters for LoRA \u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Train to align species embeddings to frozen embeddings\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mfinetuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspecies_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrozen_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#TODO - set frozen embeddings\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Extract tuned embeddings\u001b[39;00m\n\u001b[1;32m     22\u001b[0m tuned_embeddings \u001b[38;5;241m=\u001b[39m finetuner\u001b[38;5;241m.\u001b[39membed(species_batch, sequence_batch)\n",
      "File \u001b[0;32m~/Olive/Kode/ZooTransform/src/zootransform/fine_tuning/fine_tuning.py:72\u001b[0m, in \u001b[0;36mLoraFinetuner.train\u001b[0;34m(self, species_batch, sequence_batch, frozen_embeddings, epochs)\u001b[0m\n\u001b[1;32m     70\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     71\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m---> 72\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# mean pooling\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Optional: align to frozen embeddings\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frozen_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frozen_embeddings) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MaskedLMOutput' object has no attribute 'last_hidden_state'"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens_old = {k: v[:max_len].reshape(-1, batch_size).to(device) for k,v in tokens_old.items()}\n",
    "# with torch.no_grad():\n",
    "#     old_embeddings = old_model(**tokens_old).last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Species-aware model\n",
    "species_model = SpeciesAwareESM2(species_list=species_names)\n",
    "\n",
    "# LoRA finetuner\n",
    "finetuner = LoraFinetuner(\n",
    "    base_model=species_model, \n",
    "    r=8, \n",
    "    alpha=16, \n",
    "    dropout=0.05, \n",
    "    target_modules=None, \n",
    "    lr=1e-4, \n",
    "    batch_size=4)  #TODO - optionally optimize parameters for LoRA \n",
    "\n",
    "# Train to align species embeddings to frozen embeddings\n",
    "finetuner.train(species_train, seq_train, frozen_embeddings=None, epochs=5) #TODO - set frozen embeddings\n",
    "\n",
    "# Extract tuned embeddings\n",
    "tuned_embeddings = finetuner.embed(species_batch, sequence_batch)\n",
    "print(\"Tuned embeddings shape:\", tuned_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a82867671cd27",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fine-tune the Model using LoRA for Masked Language Modeling (MLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcaba205c902c30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T15:37:20.449204Z",
     "start_time": "2025-11-05T15:37:19.370155Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "  GPU: NVIDIA GeForce RTX 4090\n",
      "  Memory: 25.39 GB\n",
      "Loading model: facebook/esm2_t6_8M_UR50D\n",
      "Adding species tokens: ['<sp_Arabidopsis thaliana>', '<sp_Bos taurus>', '<sp_Escherichia coli>', '<sp_Homo sapiens>', '<sp_Mus musculus>', '<sp_Oryza sativa>', '<sp_Rattus norvegicus>', '<sp_Rhodotorula toruloides>', '<sp_Saccharolobus solfataricus>', '<sp_Saccharomyces cerevisiae>', '<sp_Schizosaccharomyces pombe>', '<sp_Staphylococcus aureus>']\n",
      "Added 12 new special tokens\n",
      "Resized model embeddings to 45 tokens\n",
      "✓ Model and tokenizer ready!\n",
      "  Hidden size: 320\n",
      "  Number of layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 30024/30024 [35:52<00:00, 13.95it/s, loss=2.0185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — avg loss: 2.3681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 30024/30024 [37:12<00:00, 13.45it/s, loss=2.3377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — avg loss: 2.3601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 30024/30024 [36:54<00:00, 13.56it/s, loss=2.4503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — avg loss: 2.3580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 30024/30024 [38:12<00:00, 13.10it/s, loss=2.2672]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — avg loss: 2.3561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 30024/30024 [35:39<00:00, 14.03it/s, loss=2.2964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — avg loss: 2.3556\n"
     ]
    }
   ],
   "source": [
    "from src.zootransform.fine_tuning.fine_tuning import LoraFinetunerMLM  # new MLM version\n",
    "from src.zootransform.model.species_model import SpeciesAwareESM2\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Species-aware model\n",
    "species_model = SpeciesAwareESM2(species_list=species_names)\n",
    "species_model.model.to(device)\n",
    "\n",
    "species_batch = data[\"species\"].tolist()\n",
    "sequence_batch = data[\"sequence\"].tolist()\n",
    "\n",
    "finetuner = LoraFinetunerMLM(\n",
    "    base_model=species_model,\n",
    "    r=8,\n",
    "    alpha=16,\n",
    "    dropout=0.05,\n",
    "    target_modules=[\"attention.self.key\", \"attention.self.value\", \"attention.self.query\", \"embeddings.word_embeddings\"],  # LoRA targets\n",
    "    lr=1e-4,\n",
    "    batch_size=4,\n",
    "    mlm_probability=0.15  # fraction of tokens to mask\n",
    ")\n",
    "\n",
    "finetuner.train(\n",
    "    species_batch=species_batch,\n",
    "    sequence_batch=sequence_batch,\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "tuned_embeddings = finetuner.embed(species_batch, sequence_batch)\n",
    "print(\"Tuned embeddings shape:\", tuned_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93817aa02b932aa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Save and Load the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d555f0f7f68947f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T15:37:20.704814Z",
     "start_time": "2025-11-05T15:37:20.451288Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapters saved to lora_finetuned_species_model\n"
     ]
    }
   ],
   "source": [
    "# Directory to save LoRA adapters\n",
    "save_dir = f\"lora_finetuned_species_model_max\" #TODO - specify path\n",
    "\n",
    "# Save only LoRA weights \n",
    "finetuner.model.save_pretrained(save_dir)\n",
    "np.save(os.path.join(save_dir, f\"tuned_embeddings_max.npy\"), tuned_embeddings)\n",
    "print(f\"LoRA adapters saved to {save_dir}\")\n",
    "\n",
    "finetuner.tokenizer.save_pretrained(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd9208b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
