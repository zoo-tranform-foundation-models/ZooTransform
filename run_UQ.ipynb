{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78717514",
   "metadata": {},
   "source": [
    "# Uncertainty Quantification for species model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4cb9d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hslab/Olive/Kode/ZooTransform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hslab/Olive/Kode/ZooTransform/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "print(os.getcwd())\n",
    "data_root = os.path.join(os.getcwd(), 'ZooTransform')\n",
    "\n",
    "sys.path.append(data_root)\n",
    "\n",
    "from src.zootransform.fine_tuning.fine_tuning import LoraFinetunerMLM  # new MLM version\n",
    "from src.zootransform.model.species_model import SpeciesAwareESM2\n",
    "from src.zootransform.dataset.load_uniprot import load_uniprot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b173be6",
   "metadata": {},
   "source": [
    "# Load data & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65982416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "  GPU: NVIDIA GeForce RTX 4090\n",
      "  Memory: 25.39 GB\n",
      "Loading model: facebook/esm2_t6_8M_UR50D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding species tokens: ['<sp_Arabidopsis thaliana>', '<sp_Bos taurus>', '<sp_Escherichia coli>', '<sp_Homo sapiens>', '<sp_Mus musculus>', '<sp_Oryza sativa>', '<sp_Rattus norvegicus>', '<sp_Rhodotorula toruloides>', '<sp_Saccharolobus solfataricus>', '<sp_Saccharomyces cerevisiae>', '<sp_Schizosaccharomyces pombe>', '<sp_Staphylococcus aureus>']\n",
      "Added 12 new special tokens\n",
      "Resized model embeddings to 45 tokens\n",
      "✓ Model and tokenizer ready!\n",
      "  Hidden size: 320\n",
      "  Number of layers: 6\n"
     ]
    }
   ],
   "source": [
    "data = load_uniprot()\n",
    "species_names = sorted(set(data['species'].unique().tolist()))\n",
    "species_model = SpeciesAwareESM2(model_name=\"facebook/esm2_t6_8M_UR50D\", species_list=species_names) #TODO - define species list\n",
    "\n",
    "# Prep dataset\n",
    "np.random.seed(0)\n",
    "n_max_dataset = 1000\n",
    "idxs_rand = np.random.choice(len(data), n_max_dataset, replace=False)\n",
    "species_batch = data[\"species\"].iloc[idxs_rand].tolist()\n",
    "sequence_batch = data[\"sequence\"].iloc[idxs_rand].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef94247d",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3142cbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "  GPU: NVIDIA GeForce RTX 4090\n",
      "  Memory: 25.39 GB\n",
      "Loading model: facebook/esm2_t6_8M_UR50D\n",
      "Adding species tokens: ['<sp_Arabidopsis thaliana>', '<sp_Bos taurus>', '<sp_Escherichia coli>', '<sp_Homo sapiens>', '<sp_Mus musculus>', '<sp_Oryza sativa>', '<sp_Rattus norvegicus>', '<sp_Rhodotorula toruloides>', '<sp_Saccharolobus solfataricus>', '<sp_Saccharomyces cerevisiae>', '<sp_Schizosaccharomyces pombe>', '<sp_Staphylococcus aureus>']\n",
      "Added 12 new special tokens\n",
      "Resized model embeddings to 45 tokens\n",
      "✓ Model and tokenizer ready!\n",
      "  Hidden size: 320\n",
      "  Number of layers: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 250/250 [00:24<00:00, 10.27it/s, loss=2.4151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — avg loss: 2.4091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 250/250 [00:24<00:00, 10.39it/s, loss=2.2072]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 — avg loss: 2.3938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 250/250 [00:24<00:00, 10.39it/s, loss=2.3366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 — avg loss: 2.3951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 250/250 [00:24<00:00, 10.39it/s, loss=2.4724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 — avg loss: 2.3924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 250/250 [00:24<00:00, 10.40it/s, loss=2.3325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 — avg loss: 2.3771\n",
      "Tuned embeddings shape: torch.Size([1000, 320])\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Species-aware model\n",
    "species_model = SpeciesAwareESM2(species_list=species_names)\n",
    "species_model.model.to(device)\n",
    "\n",
    "finetuner = LoraFinetunerMLM(\n",
    "    base_model=species_model,\n",
    "    r=8,\n",
    "    alpha=16,\n",
    "    dropout=0.05,\n",
    "    target_modules=[\"attention.self.key\", \"attention.self.value\", \"attention.self.query\", \"embeddings.word_embeddings\"],  # LoRA targets\n",
    "    lr=1e-4,\n",
    "    batch_size=4,\n",
    "    mlm_probability=0.15  # fraction of tokens to mask\n",
    ")\n",
    "\n",
    "finetuner.train(\n",
    "    species_batch=species_batch,\n",
    "    sequence_batch=sequence_batch,\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "tuned_embeddings = finetuner.embed(species_batch, sequence_batch)\n",
    "print(\"Tuned embeddings shape:\", tuned_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b51049f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapters saved to lora_finetuned_species_model_1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('lora_finetuned_species_model_1000/tokenizer_config.json',\n",
       " 'lora_finetuned_species_model_1000/special_tokens_map.json',\n",
       " 'lora_finetuned_species_model_1000/vocab.txt',\n",
       " 'lora_finetuned_species_model_1000/added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directory to save LoRA adapters\n",
    "save_dir = f\"lora_finetuned_species_model_{int(n_max_dataset)}\" #TODO - specify path\n",
    "\n",
    "# Save only LoRA weights \n",
    "finetuner.model.save_pretrained(save_dir)\n",
    "np.save(os.path.join(save_dir, f\"tuned_embeddings_{int(n_max_dataset)}.npy\"), tuned_embeddings)\n",
    "print(f\"LoRA adapters saved to {save_dir}\")\n",
    "\n",
    "finetuner.tokenizer.save_pretrained(save_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd1240",
   "metadata": {},
   "source": [
    "# Run uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9583802",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EsmForMaskedLM' object has no attribute 'embed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m species_uq \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecies\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[idxs_rand]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      5\u001b[0m sequence_uq \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[idxs_rand]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m----> 7\u001b[0m mean_embedding, uncertainty, embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mspecies_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_with_uncertainty\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspecies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspecies_uq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_uq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_mc_draws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Olive/Kode/ZooTransform/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Olive/Kode/ZooTransform/src/zootransform/model/species_model.py:227\u001b[0m, in \u001b[0;36mSpeciesAwareESM2.embed_with_uncertainty\u001b[0;34m(self, species, sequences, n_mc_draws)\u001b[0m\n\u001b[1;32m    224\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_mc_draws):\n\u001b[0;32m--> 227\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m(species, sequences)\n\u001b[1;32m    228\u001b[0m     emb_mean \u001b[38;5;241m=\u001b[39m emb\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    229\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(emb_mean)\n",
      "File \u001b[0;32m~/Olive/Kode/ZooTransform/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1940\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1939\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1940\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1942\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EsmForMaskedLM' object has no attribute 'embed'"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "n_max_dataset_uq = 2000\n",
    "idxs_rand = np.random.choice(len(data), n_max_dataset_uq, replace=False)\n",
    "species_uq = data[\"species\"].iloc[idxs_rand].tolist()\n",
    "sequence_uq = data[\"sequence\"].iloc[idxs_rand].tolist()\n",
    "\n",
    "mean_embedding, uncertainty, embeddings = species_model.embed_with_uncertainty(\n",
    "    species=species_uq,\n",
    "    sequences=sequence_uq,\n",
    "    n_mc_draws=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
